volumes:
  caddy-data:
    name: caddy-data
  caddy-config:
    name: caddy-config
  database:
    name: database
  relay:
    name: relay
  bsky:
    name: bsky
  redis:
    name: redis
  zplc-data:
    name: zplc-data
  labelmuncher:
    name: labelmuncher

networks:
  default:
    name: ${DOCKER_NETWORK_NAME:-bluesky-network}
    external: true

services:
  caddy:
    # cf. https://blog.kurokobo.com/archives/3669#Caddy_acme_server
    image: caddy:2
    ports:
      - 80:80
      - 443:443
      - 443:443/udp
      - 9000:9000
    environment:
      - DOMAIN=${DOMAIN:?}
      - EMAIL_FOR_CERTS=${EMAIL_FOR_CERTS:?}
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile
      - caddy-data:/data
      - caddy-config:/config
    healthcheck:
      # https://caddy.community/t/what-is-the-best-practise-for-doing-a-health-check-for-caddy-containers/12995
      test: "wget --no-verbose --tries=1 --spider http://localhost:2019/metrics || exit 1"
      interval: 5s
      retries: 20

  database:
    # this is a custom image that includes the pg_repack extension
    # you probably won't need it unless you end up in my situation, where I discover after backfilling
    # that my disk is nearly full, and shutting everything down to vacuum would take longer than relay playback allows
    image: ${IMAGE_NAMESPACE:?}/postgres
    build:
      dockerfile_inline: |
        FROM postgres:16

        RUN apt-get update --fix-missing && \
            apt-get install -y postgresql-server-dev-16 wget openssh-server

        RUN apt-get install -y make unzip gcc libssl-dev zlib1g-dev liblz4-dev libreadline-dev libzstd-dev
        RUN wget -q -O pg_repack.zip "https://api.pgxn.org/dist/pg_repack/1.5.2/pg_repack-1.5.2.zip"
        RUN unzip pg_repack.zip && rm pg_repack.zip
        RUN cd pg_repack-* && make && make install
        RUN rm -rf pg_repack-*
        RUN apt-get remove --auto-remove -y make unzip gcc libssl-dev zlib1g-dev libreadline-dev libzstd-dev
    pull_policy: never
    shm_size: 16gb
    command:
      # these settings are based on a 128GB RAM machine with 32 cores, adjust as needed
      # work_mem, wal_buffers, shared_buffers, max_*_workers should be increased when backfilling
      "-c 'max_connections=1000' -c 'idle_in_transaction_session_timeout=60000' -c 'statement_timeout=300000' -c 'lock_timeout=2000' -c 'search_path=bsky,public,pg_catalog'
      -c 'max_wal_size=16GB' -c 'shared_buffers=24GB' -c 'maintenance_work_mem=2GB' -c 'work_mem=32MB' -c 'wal_buffers=36MB' -c 'effective_cache_size=80GB'
      -c 'max_parallel_workers=64' -c 'max_parallel_workers_per_gather=8' -c 'max_worker_processes=64' -c 'max_parallel_maintenance_workers=32' -c 'autovacuum_max_workers=8'
      -c 'autovacuum_vacuum_scale_factor=0.05' -c 'autovacuum_analyze_scale_factor=0.02' -c 'autovacuum_vacuum_threshold=1000' -c 'autovacuum_analyze_threshold=500'
      -c 'checkpoint_completion_target=0.9' -c 'max_locks_per_transaction=256' -c 'default_statistics_target=100' -c 'random_page_cost=1.1' -c 'effective_io_concurrency=200'
      -c 'log_min_duration_statement=400' -c 'log_min_error_statement=warning' -c 'log_temp_files=0' -c 'log_lock_waits=true' -c 'log_error_verbosity=verbose' -c 'log_rotation_size=40MB'
      -c 'shared_preload_libraries=pg_stat_statements' -c 'pg_stat_statements.track=all'"
    environment:
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
      - POSTGRES_USER=${POSTGRES_USER:-pg}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:?}
      - PGPORT=5432
      - POSTGRES_DB=healthcheck
    volumes:
      - ./config/init-postgres:/docker-entrypoint-initdb.d/
      - database:/var/lib/postgresql/data/
    restart: always

  pgbouncer:
    image: bitnami/pgbouncer:latest
    ports:
      - 5432:5432
    environment:
      - POSTGRESQL_HOST=database
      - POSTGRESQL_PORT=5432
      - POSTGRESQL_USERNAME=${POSTGRES_USER:-pg}
      - POSTGRESQL_PASSWORD=${POSTGRES_PASSWORD:?}
      - PGBOUNCER_DATABASE=*
      - PGBOUNCER_AUTH_TYPE=md5
      - PGBOUNCER_AUTH_USER=${POSTGRES_USER:-pg}
      - PGBOUNCER_SET_DATABASE_USER=true
      - PGBOUNCER_SET_DATABASE_PASSWORD=true
      - PGBOUNCER_LISTEN_ADDRESS=0.0.0.0
      - PGBOUNCER_PORT=5432
      - PGBOUNCER_POOL_MODE=transaction
      - PGBOUNCER_DEFAULT_POOL_SIZE=100
      - PGBOUNCER_RESERVE_POOL_SIZE=50
      - PGBOUNCER_MIN_POOL_SIZE=0
      - PGBOUNCER_MAX_CLIENT_CONN=10000
      - PGBOUNCER_MAX_PREPARED_STATEMENTS=1000
      - PGBOUNCER_EXTRA_FLAGS=-v
    depends_on:
      - database
    restart: always

  redis:
    image: redis:7-bookworm
    volumes:
      - redis:/data/
    restart: always

  # runs https://github.com/char/zplc-server/blob/main/src/ingest.ts
  zplc-ingest:
    image: ${IMAGE_NAMESPACE:?}/zplc
    build:
      context: ./repos/zplc-server/
      dockerfile_inline: |
        FROM denoland/deno:latest
        WORKDIR /plc
        COPY --chown=deno:deno . .
    pull_policy: never
    command: deno run -A src/ingest.ts
    volumes:
      - zplc-data:/plc/data
    restart: always

  # runs https://github.com/char/zplc-server/blob/main/src/serve-plc.ts
  zplc-server:
    image: ${IMAGE_NAMESPACE:?}/zplc
    build:
      context: ./repos/zplc-server/
      dockerfile_inline: |
        FROM denoland/deno:latest
        WORKDIR /plc
        COPY --chown=deno:deno . .
    pull_policy: never
    command: deno serve -A --parallel --port 2582 src/serve-plc.ts
    ports:
      - 2582:2582
    volumes:
      - zplc-data:/plc/data
    restart: always

  # relay
  relay:
    image: ${IMAGE_NAMESPACE:?}/relay
    ports:
      - 2470:2470
    build:
      context: ./repos/indigo/
      dockerfile: cmd/relay/Dockerfile
    command: /relay
    environment:
      - RELAY_ADMIN_KEY=${RELAY_ADMIN_KEY:?}
      - RELAY_PLC_HOST=https://${PLC_FQDN:-plc.${DOMAIN}}
      - RELAY_DEFAULT_REPO_LIMIT=700_000
      - RELAY_EVENT_PLAYBACK_TTL=48h
      - RELAY_NEW_HOSTS_PER_DAY_LIMIT=9999
      - RELAY_DEFAULT_ACCOUNT_LIMIT=100 # max accounts per new PDS
      - DATABASE_URL=postgres://${POSTGRES_USER:-pg}:${POSTGRES_PASSWORD:?}@pgbouncer/relay
    volumes:
      - relay:/data
    restart: always
    depends_on:
      - database
      - pgbouncer
      - caddy
      - zplc-server

  # the appview
  bsky:
    image: ${IMAGE_NAMESPACE:?}/bsky
    build:
      context: ./repos/atproto/
      dockerfile: services/bsky/Dockerfile
    ports:
      - 2584:2584
    expose:
      - 2584
    command: node --enable-source-maps api.js
    user: root
    environment:
      - BSKY_ADMIN_PASSWORDS=${BSKY_ADMIN_PASSWORDS}
      - BSKY_BLOB_CACHE_LOC=/cache/
      - BSKY_BSYNC_HTTP_VERSION=1.1
      - BSKY_BSYNC_PORT=3002
      - BSKY_BSYNC_URL=http://bsky:3002
      - BSKY_DATAPLANE_HTTP_VERSION=1.1
      - BSKY_DATAPLANE_PORT=3001
      - BSKY_DATAPLANE_URLS=http://bsky:3001
      - BSKY_DB_POSTGRES_SCHEMA=bsky
      - BSKY_DB_POSTGRES_URL=postgres://${POSTGRES_USER:-pg}:${POSTGRES_PASSWORD:?}@pgbouncer/bsky
      - BSKY_DB_POOL_SIZE=500
      - BSKY_DID_PLC_URL=https://${PLC_FQDN:-plc.${DOMAIN}}
      - BSKY_LABELS_FROM_ISSUER_DIDS=${BSKY_LABELS_FROM_ISSUER_DIDS}
      - BSKY_PUBLIC_URL=https://${BSKY_FQDN:-bsky.${DOMAIN}}
      - BSKY_REPO_PROVIDER=wss://${RELAY_FQDN:-relay.${DOMAIN}}
      - BSKY_SERVER_DID=did:web:${BSKY_FQDN:-bsky.${DOMAIN}}
      - BSKY_SERVICE_SIGNING_KEY=${BSKY_SERVICE_SIGNING_KEY:?}
      - BSKY_INDEXED_AT_EPOCH=${BSKY_INDEXED_AT_EPOCH:-2022-11-17T00:35:16.391+00:00}
      - BSKY_VIDEO_PLAYLIST_URL_PATTERN=https://video.bsky.app/watch/%s/%s/playlist.m3u8
      - BSKY_VIDEO_THUMBNAIL_URL_PATTERN=https://video.bsky.app/watch/%s/%s/thumbnail.jpg
      - MOD_SERVICE_DID=${MOD_SERVICE_DID}
      - CLUSTER_WORKER_COUNT=3
      - DEBUG_MODE=1
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV:-production}
      - NODE_OPTIONS=--max-old-space-size=32768
      - REDIS_HOST=redis
      - PORT=2584
    volumes:
      - bsky:/cache/
    restart: always
    depends_on:
      - database
      - pgbouncer
      - redis
      - caddy

  # indexes from relay into appview database
  # https://github.com/zeppelin-social/bsky-indexer
  bsky-indexer:
    init: true
    image: ${IMAGE_NAMESPACE:?}/bsky-indexer
    build:
      context: ./repos/bsky-indexer/
    ports:
      - 9229:9229
    command: deno run --allow-all src/bin/bin.ts
    user: root
    environment:
      - BSKY_DB_POSTGRES_SCHEMA=bsky
      - BSKY_DB_POSTGRES_URL=postgres://${POSTGRES_USER:-pg}:${POSTGRES_PASSWORD:?}@pgbouncer/bsky
      - BSKY_DB_POOL_SIZE=50
      - BSKY_REPO_PROVIDER=wss://${BGS_FQDN:-relay1.us-east.bsky.network}
      - DID_PLC_URL=https://${PLC_FQDN:-plc.${DOMAIN}}
      - REDIS_URL=redis://redis:6379
      - SUB_MIN_WORKERS=16 # you can get by with 8 or fewer off-peak, but they're pretty cheap
      - SUB_MAX_WORKERS=64 # you need at least ~30-40 to keep up with backfill if you miss a few hours
      - STATS_FREQUENCY_MS=60000
    restart: always
    depends_on:
      - database
      - pgbouncer
      - caddy

  # consumes labels from specified labelers into appview database
  # https://github.com/zeppelin-social/bsky-indexer
  labelmuncher:
    image: ${IMAGE_NAMESPACE:?}/labelmuncher
    build:
      context: ./repos/labelmuncher/
    pull_policy: never
    environment:
      - BSKY_DATAPLANE_HTTP_VERSION=1.1
      - BSKY_DATAPLANE_URLS=http://bsky:3001
      - BSKY_DB_POSTGRES_SCHEMA=bsky
      - BSKY_DB_POSTGRES_URL=postgres://${POSTGRES_USER:-pg}:${POSTGRES_PASSWORD:?}@pgbouncer/bsky
      - BSKY_DID_PLC_URL=https:/plc.directory # if you're reading this, you can probably change to ${PLC_FQDN}, my setup is just messed up
      - BSKY_LABELS_FROM_ISSUER_DIDS=${BSKY_LABELS_FROM_ISSUER_DIDS}
      - MOD_SERVICE_DID=${MOD_SERVICE_DID}
    restart: always
    volumes:
      - labelmuncher:/app
    depends_on:
      - database
      - pgbouncer
      - caddy

  # (optional) social-app instance to go with your appview
  social-app:
    image: ${IMAGE_NAMESPACE:?}/social-app
    build:
      context: ./repos/social-app/
    ports:
      - 8100:8100
    command: /usr/bin/bskyweb serve
    environment:
      - ATP_APPVIEW_HOST=https://${BSKY_FQDN:-bsky.${DOMAIN}}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - HTTP_ADDRESS=:8100
      - NODE_ENV=${NODE_ENV:-production}
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl http://localhost:8100 | grep 'API docs at https://atproto.com' >/dev/null",
        ]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always
